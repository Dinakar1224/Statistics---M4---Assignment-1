{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tExplain the properties of the F-distribution.**"
      ],
      "metadata": {
        "id": "1pZUPo2w39Qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Properties of the F-Distribution\n",
        "The F-distribution is a continuous probability distribution that arises primarily in the context of comparing variances between different groups or samples. It is used in various statistical tests, including ANOVA (Analysis of Variance), regression analysis, and testing the equality of variances.\n",
        "Here are the key properties of the F-distribution:\n",
        "\n",
        "1. Shape and Asymmetry\n",
        "The F-distribution is right-skewed, meaning it has a longer tail on the right side. This is because it’s based on the ratio of two squared quantities (variances), which cannot be negative, leading to a distribution that starts at zero and extends to positive infinity.\n",
        "As the degrees of freedom (df) for the numerator and denominator increase, the distribution becomes more symmetric and approaches a normal distribution.\n",
        "\n",
        "2. Degrees of Freedom\n",
        "The F-distribution is defined by two degrees of freedom:\n",
        "Numerator degrees of freedom (df1df_1df1): This refers to the degrees of freedom associated with the variance estimate in the numerator (e.g., the variance of the first sample or group).\n",
        "Denominator degrees of freedom (df2df_2df2): This refers to the degrees of freedom associated with the variance estimate in the denominator (e.g., the variance of the second sample or group).\n",
        "The shape of the F-distribution depends heavily on these two degrees of freedom. The distribution will look different based on the values of df1df_1df1 and df2df_2df2.\n",
        "\n",
        "3. Range\n",
        "The range of the F-distribution is from 0 to ∞. Since the F-statistic is a ratio of squared terms (variances), it cannot take negative values. As the values of the variances (numerator or denominator) increase, the F-statistic also increases.\n",
        "\n",
        "\n",
        "4. Skewness and Kurtosis\n",
        "Skewness: The F-distribution is always right-skewed, especially when the degrees of freedom for both the numerator and denominator are small. The skewness decreases as both degrees of freedom increase.\n",
        "Kurtosis: The F-distribution has a positive kurtosis, meaning it has a relatively high peak and heavier tails compared to the normal distribution, especially for smaller degrees of freedom.\n",
        "\n",
        "5. Symmetry\n",
        "Unlike the normal or t-distributions, the F-distribution is not symmetric. It is always skewed to the right, especially when both degrees of freedom are small. As the degrees of freedom for the numerator and denominator grow larger, the distribution begins to look more symmetric.\n",
        "\n",
        "6. Applications\n",
        "The F-distribution is widely used in variance analysis and hypothesis testing, especially in:\n",
        "Analysis of Variance (ANOVA): To test the differences between the means of three or more groups.\n",
        "Testing the ratio of variances: For comparing the variances between two populations, such as in an F-test for the equality of variances.\n",
        "Regression analysis: In testing the overall significance of a regression model, specifically in the context of F-tests for model fit.\n",
        "\n",
        "7. Critical Values and Use in Hypothesis Testing\n",
        "The critical values for the F-distribution depend on both the numerator and denominator degrees of freedom, as well as the significance level (α\\alphaα) of the test.\n",
        "F-tests are used to determine whether there are significant differences between the variances of two or more groups. For example, in ANOVA, the F-statistic is used to test whether the group means are equal by comparing the variances within and between groups.\n",
        "The p-value associated with the F-statistic is used to determine the significance of the test. If the F-statistic is larger than the critical value from the F-distribution table (for a given significance level), you reject the null hypothesis.\n",
        "\n",
        "8. Relationship with Chi-Square Distribution\n",
        "The F-distribution can be derived from the Chi-square distribution. Specifically, if you divide two independent chi-square distributed variables by their respective degrees of freedom, the resulting ratio follows an F-distribution.\n"
      ],
      "metadata": {
        "id": "ARIj86de3-6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.\tIn which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**"
      ],
      "metadata": {
        "id": "tSYRjKTO3-9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is primarily used in the following types of statistical tests:\n",
        "1.\tAnalysis of Variance (ANOVA):\n",
        "Purpose: ANOVA tests are used to compare the means of three or more groups to determine if at least one group mean is different from the others.\n",
        "Why the F-distribution is used: In ANOVA, the F-statistic is the ratio of the variance between groups (between-group variability) to the variance within groups (within-group variability). Since variances follow a chi-squared distribution, and the F-statistic is the ratio of two such distributions, it follows the F-distribution. The F-distribution is appropriate here because it helps assess if the observed variation between group means is larger than what would be expected due to random sampling error.\n",
        "2.\tRegression Analysis:\n",
        "Purpose: In multiple regression analysis, the F-test is used to assess the overall significance of a regression model.\n",
        "Why the F-distribution is used: The F-statistic is computed by comparing the variance explained by the regression model to the unexplained variance. A large F-statistic suggests that the model explains a significant amount of the variability in the dependent variable. The F-distribution is used here because it arises from the ratio of variances from the regression model and the residuals.\n",
        "3.\tTesting Equality of Variances (e.g., in a two-sample test for variance):\n",
        "Purpose: The F-test is used to compare the variances of two populations or samples to test if they are equal.\n",
        "Why the F-distribution is used: When comparing the variances of two independent samples, the ratio of their sample variances follows an F-distribution. This test is appropriate because the F-statistic is the ratio of two independent chi-squared variables (which follow a scaled chi-squared distribution), and this ratio follows an F-distribution.\n",
        "4.\tGeneral Linear Models (GLM) and ANOVA for More Complex Models:\n",
        "Purpose: The F-distribution is used for testing the significance of various factors in more complex statistical models like GLM, which generalizes ANOVA.\n",
        "Why the F-distribution is used: In GLMs, the F-statistic tests the overall fit of the model by comparing model fit with a baseline model. The distribution of this test statistic follows the F-distribution.\n",
        "Why is the F-distribution appropriate for these tests?\n",
        "The F-distribution is appropriate for these tests because:\n",
        "It is derived from the ratio of two chi-squared distributed variables, which are used to estimate variances.\n",
        "The F-distribution is positively skewed, with values starting at 0 and extending to positive infinity. This matches the behavior of variances and their ratios in real data.\n",
        "It has two parameters: degrees of freedom for the numerator and the denominator, which reflect the degrees of freedom associated with the two variances being compared. These parameters help control the shape of the distribution based on the number of groups or predictors involved.\n"
      ],
      "metadata": {
        "id": "OyhvURAP3-_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?**"
      ],
      "metadata": {
        "id": "hoHnQ7U43_Bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ans: When conducting an F-test to compare the variances of two populations, several key assumptions must be met for the results to be valid. These assumptions are important to ensure the test statistic follows an F-distribution and that the conclusions drawn are accurate. The key assumptions are:\n",
        "1. Independence of Samples\n",
        "The two samples being compared should be independent of each other. This means that the values in one sample should not provide any information about the values in the other sample. This assumption ensures that the variability between the two groups is not influenced by any correlation between them.\n",
        "2. Normality of the Populations\n",
        "Both populations from which the samples are drawn should be normally distributed (or at least approximately normal). The F-test assumes that the two populations follow a normal distribution because the variance of a normally distributed population is meaningful and stable. If the populations are not normal, the F-test may not be valid, though the test is somewhat robust to deviations from normality if the sample sizes are large (thanks to the central limit theorem).\n",
        "3. The Samples are Randomly Selected\n",
        "The samples should be randomly selected from the populations. Random sampling ensures that each individual has an equal chance of being selected, helping to reduce bias and increase the generalizability of the results.\n",
        "4. The Populations Have Different, but Known Variances (Homogeneity of Variance)\n",
        "The F-test is used to test the null hypothesis that the two populations have equal variances. Under the null hypothesis, the two populations are assumed to have the same variance, and the test compares the ratio of their sample variances to determine if there is enough evidence to suggest that the population variances are different.\n",
        "5. The Two Samples Are From Populations with Positive Variances\n",
        "Both populations must have positive variances, as variance cannot be negative. This assumption is fundamental since the F-statistic is the ratio of two sample variances, and variance estimates cannot logically be negative.\n",
        "**Summary of Assumptions:**\n",
        "1.\tIndependence: The samples must be independent of each other.\n",
        "2.\tNormality: The populations must be approximately normally distributed.\n",
        "3.\tRandom Sampling: The samples should be randomly selected.\n",
        "4.\tHomogeneity of Variance: The populations should have equal variances under the null hypothesis.\n",
        "5.\tPositive Variance: The variances should be positive.\n"
      ],
      "metadata": {
        "id": "e-AP7OZD3_EK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.What is the purpose of ANOVA, and how does it differ from a t-test**"
      ],
      "metadata": {
        "id": "LExPET9S3_GT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: Purpose of ANOVA (Analysis of Variance):\n",
        "ANOVA is a statistical test used to compare the means of three or more groups to determine if at least one group mean is statistically different from the others. It assesses whether the variation within each group is significantly smaller than the variation between the groups, allowing researchers to test hypotheses about group differences.\n",
        "The key purpose of ANOVA is to determine if there are any statistically significant differences between the means of multiple groups without increasing the risk of Type I errors (false positives) that would occur if multiple t-tests were used.\n",
        " Difference Between ANOVA and a t-test:\n",
        "The t-test and ANOVA are both used to compare means, but they are suited for different scenarios:\n",
        "1.\tNumber of Groups Compared:\n",
        "t-test: Used to compare the means of two groups (e.g., comparing the test scores of two groups of students).\n",
        "ANOVA: Used to compare the means of three or more groups (e.g., comparing test scores across three different teaching methods).\n",
        "2.\tType of Hypothesis:\n",
        "t-test: Tests whether the mean of one group is significantly different from the mean of another group.\n",
        "ANOVA: Tests whether there is a significant difference among the means of three or more groups. It doesn't tell which specific groups differ but identifies whether at least one group differs.\n",
        "3.\tRisk of Type I Errors:\n",
        "t-test: If multiple t-tests are performed (e.g., testing differences between multiple pairs of groups), the Type I error rate increases (i.e., the probability of incorrectly rejecting the null hypothesis increases). This is because each test carries its own chance of error, and multiple tests compound this risk.\n",
        "ANOVA: By comparing all groups simultaneously in one test, ANOVA controls for the overall Type I error rate, making it a more reliable method when dealing with multiple groups.\n",
        "t-test: The test statistic is a t-statistic, which is calculated by dividing the difference between the two group means by the standard error of the difference.\n",
        "ANOVA: The test statistic is an F-statistic, which is the ratio of the variance between the groups to the variance within the groups.\n"
      ],
      "metadata": {
        "id": "tklvC43F3_Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.**"
      ],
      "metadata": {
        "id": "ZFHE-AFu3_M2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: When comparing the means of more than two groups, a one-way ANOVA is generally preferred over using multiple t-tests. Here's an explanation of when and why you would use one over the other:\n",
        "Why One-Way ANOVA is Preferred Over Multiple t-tests:\n",
        "1.\tControlling Type I Error Rate:\n",
        "Multiple t-tests: When performing multiple t-tests to compare each pair of groups (e.g., comparing Group 1 vs. Group 2, Group 1 vs. Group 3, and Group 2 vs. Group 3), each t-test carries its own risk of a Type I error (rejecting the null hypothesis when it is actually true). The more tests you perform, the higher the probability of making a false positive (finding a difference that isn't actually there).\n",
        "Example: If you conduct 3 t-tests, each with a significance level of 0.05, the probability of making a Type I error increases beyond 5% because each test has a 5% chance of incorrectly rejecting the null hypothesis.\n",
        "Formula for cumulative Type I error: If performing kkk tests, the overall Type I error rate is 1−(1−α)k1 - (1 - \\alpha)^k1−(1−α)k, where α\\alphaα is the significance level for each individual test. This grows rapidly with more tests.\n",
        "One-Way ANOVA: ANOVA controls the overall Type I error rate by testing all groups simultaneously with one test. It tests whether at least one group mean is different from the others in a single comparison. This significantly reduces the risk of making a Type I error when comparing more than two groups.\n",
        "2.\tOverall Test for Differences:\n",
        "Multiple t-tests: When you use multiple t-tests, you are essentially testing multiple hypotheses (e.g., “Is the mean of Group 1 different from Group 2?”, “Is the mean of Group 1 different from Group 3?”, etc.). Each of these tests is a separate hypothesis test, and their individual outcomes might lead to conflicting conclusions.\n",
        "One-Way ANOVA: ANOVA provides a global test to assess whether there is any significant difference in means across the groups. It works by comparing the variation between the groups (group means) and the variation within the groups (individual data points). If the between-group variation is significantly larger than the within-group variation, ANOVA concludes that there is a significant difference in means across at least one group.\n",
        "3.\tMore Efficient and Reliable:\n",
        "Multiple t-tests: Conducting many t-tests increases the number of statistical decisions, and this can lead to inefficiency. Each t-test needs its own calculation, and handling many pairwise comparisons can become cumbersome.\n",
        "One-Way ANOVA: By combining all comparisons into a single analysis, ANOVA is much more efficient. Instead of performing k(k−1)/2k(k-1)/2k(k−1)/2 pairwise t-tests (where kkk is the number of groups), you perform just one test. This reduces computational complexity and helps to focus on the overall pattern of differences across groups.\n",
        "Example:\n",
        "Suppose you are testing the effectiveness of three different teaching methods (Group 1: Method A, Group 2: Method B, Group 3: Method C) on student performance. You want to know if there is a difference in average test scores between the methods.\n",
        "Multiple t-tests: You would perform:\n",
        "Group 1 vs. Group 2\n",
        "Group 1 vs. Group 3\n",
        "Group 2 vs. Group 3 In total, you would conduct 3 tests. If each test has a significance level of 0.05, the overall risk of Type I error increases, and you might incorrectly conclude that some group means are significantly different even if they aren't.\n",
        "One-Way ANOVA: Instead of performing 3 separate t-tests, you would perform one ANOVA test to compare the means of all three groups at once. This method reduces the risk of Type I errors and provides a more reliable result for whether there is a significant difference among the groups.\n",
        "Steps Involved in One-Way ANOVA:\n",
        "1.\tNull Hypothesis (H₀): All group means are equal (μ1=μ2=μ3\\mu_1 = \\mu_2 = \\mu_3μ1=μ2=μ3).\n",
        "2.\tAlternative Hypothesis (H₁): At least one group mean is different.\n",
        "3.\tANOVA Calculation:\n",
        "Between-group variability: Variability due to the differences in group means.\n",
        "Within-group variability: Variability within each group (due to individual differences).\n"
      ],
      "metadata": {
        "id": "KoBuszYq3_Pp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.\tExplain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?**"
      ],
      "metadata": {
        "id": "oxssZuH43_Se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: In Analysis of Variance (ANOVA), the total variance observed in the data is partitioned into two components: between-group variance and within-group variance. This partitioning allows us to assess whether the differences between the groups are significant or if they are just due to random chance.\n",
        "1. Total Variance\n",
        "The total variance refers to the overall variability in the dataset. It is calculated based on the deviation of each data point from the grand mean (the mean of all observations across all groups).\n",
        "Total Sum of Squares (SST)=∑(Yij−Yˉgrand)2\\text{Total Sum of Squares (SST)} = \\sum (Y_{ij} - \\bar{Y}_{\\text{grand}})^2Total Sum of Squares (SST)=∑(Yij−Yˉgrand)2\n",
        "Where:\n",
        "YijY_{ij}Yij is the individual observation in group jjj,\n",
        "Yˉgrand\\bar{Y}_{\\text{grand}}Yˉgrand is the grand mean.\n",
        "2. Between-group Variance (or Between-group Sum of Squares)\n",
        "The between-group variance captures how much the group means differ from the grand mean. It reflects the variability in the data due to the differences between the different groups.\n",
        "Between-group Sum of Squares (SSB)=∑nj(Yˉj−Yˉgrand)2\\text{Between-group Sum of Squares (SSB)} = \\sum n_j (\\bar{Y}_j - \\bar{Y}_{\\text{grand}})^2Between-group Sum of Squares (SSB)=∑nj(Yˉj−Yˉgrand)2\n",
        "Where:\n",
        "njn_jnj is the number of observations in group jjj,\n",
        "Yˉj\\bar{Y}_jYˉj is the mean of group jjj,\n",
        "Yˉgrand\\bar{Y}_{\\text{grand}}Yˉgrand is the grand mean.\n",
        "This term quantifies the extent to which the group means differ from the overall mean. If the group means are very different from each other, the between-group variance will be large.\n",
        "3. Within-group Variance (or Within-group Sum of Squares)\n",
        "The within-group variance measures how much the data points within each group deviate from their respective group mean. It captures the variability that is not explained by the differences between the groups.\n",
        "Within-group Sum of Squares (SSW)=∑∑(Yij−Yˉj)2\\text{Within-group Sum of Squares (SSW)} = \\sum \\sum (Y_{ij} - \\bar{Y}_j)^2Within-group Sum of Squares (SSW)=∑∑(Yij−Yˉj)2\n",
        "Where:\n",
        "YijY_{ij}Yij is the individual observation in group jjj,\n",
        "Yˉj\\bar{Y}_jYˉj is the mean of group jjj.\n",
        "This term reflects the random variability within each group. A high within-group variance suggests that the data points within each group are spread out, while a low within-group variance indicates that the data points are relatively consistent within each group.\n",
        "4. The F-statistic\n",
        "The F-statistic is a ratio of the between-group variance to the within-group variance. It is used to test the null hypothesis that all group means are equal (i.e., there is no significant difference between the groups).\n",
        "Contribution of Partitioning to the F-statistic Calculation:\n",
        "Between-group variance reflects the variability explained by the group differences (the systematic effect). A larger between-group variance relative to within-group variance suggests that the group means are significantly different from each other.\n",
        "Within-group variance reflects the random variability that exists within each group. A smaller within-group variance suggests that the data points within each group are relatively homogeneous.\n",
        "The F-statistic compares these two sources of variance. If the between-group variance is large compared to the within-group variance, the F-statistic will be large, which suggests that there is a significant difference between the group means. If the F-statistic is small (close to 1), it suggests that the differences between the groups are not significantly greater than the random variability within the groups.\n"
      ],
      "metadata": {
        "id": "KkD1YHK83_VG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**"
      ],
      "metadata": {
        "id": "i1X4nbhF3_Xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: The classical (frequentist) approach and the Bayesian approach to ANOVA differ significantly in their treatment of uncertainty, parameter estimation, and hypothesis testing. Below is a comparison of these two approaches:\n",
        "1. Handling of Uncertainty\n",
        "Classical (Frequentist) Approach: In the frequentist approach, uncertainty is primarily viewed in terms of the sampling process. The data are treated as fixed, and the parameters (such as group means or variances) are considered unknown but fixed values that we are trying to estimate based on the observed data. The uncertainty in the estimates is quantified by the standard errors or confidence intervals, which describe the variability in the estimates based on repeated sampling.\n",
        "Bayesian Approach: The Bayesian approach treats uncertainty as a probability distribution over possible parameter values. In this approach, the parameters are considered random variables with prior distributions reflecting our beliefs about the parameters before observing the data. After observing the data, the uncertainty about the parameters is updated through Bayes' theorem, resulting in a posterior distribution. This posterior distribution reflects our updated knowledge about the parameters, incorporating both the prior beliefs and the observed data.\n",
        "2. Parameter Estimation\n",
        "Classical (Frequentist) Approach: The frequentist approach estimates parameters using point estimates (such as sample means) and maximum likelihood estimation (MLE). For ANOVA, the estimates for the group means, variances, and the overall model parameters are derived by minimizing or maximizing certain functions (such as the sum of squared deviations). Confidence intervals can be constructed around these estimates to provide a range of plausible values for the parameters, but the estimates themselves are fixed.\n",
        "Bayesian Approach: In Bayesian ANOVA, parameters are estimated through the posterior distribution. Instead of a single point estimate, the Bayesian approach provides a full distribution over possible parameter values. For instance, the group means and variances are treated as random variables with their own distributions, and the posterior distribution can be summarized by means such as the posterior mean, median, or credible intervals (the Bayesian equivalent of confidence intervals). This allows for more comprehensive uncertainty quantification about the parameters.\n",
        "3. Hypothesis Testing\n",
        "Classical (Frequentist) Approach: In the frequentist approach, hypothesis testing is done using p-values. For ANOVA, the null hypothesis typically states that the group means are equal, and the alternative hypothesis suggests that at least one group mean is different. The p-value measures the probability of observing the data, or something more extreme, if the null hypothesis were true. If the p-value is less than a significance level (typically 0.05), the null hypothesis is rejected.\n",
        " and the rejection of the null hypothesis is based on the sampling distribution of the test statistic.\n",
        "Bayesian Approach: Bayesian hypothesis testing is often framed in terms of model comparison and Bayes factors. Rather than testing a single null hypothesis, Bayesian testing compares the likelihood of the data under different models. The Bayes factor is the ratio of the posterior odds of two models (e.g., one model assuming equal group means vs. a model allowing for differences in means). A higher Bayes factor indicates stronger evidence for one model over the other.\n",
        "Bayesian hypothesis testing can be more flexible, as it allows for direct comparisons of the probabilities of various hypotheses or models. This can be especially useful when the frequentist p-value is difficult to interpret or when multiple hypotheses are considered simultaneously.\n",
        "4. Model Assumptions\n",
        "Classical (Frequentist) Approach: In the classical ANOVA framework, certain assumptions need to be made about the data, such as normality of the residuals, homogeneity of variance, and independence of observations. These assumptions influence the validity of the F-statistic and other tests. If these assumptions are violated, the results of the ANOVA may be misleading.\n",
        "The frequentist approach typically uses point estimates and focuses on testing whether the assumptions hold, often using diagnostic plots or tests for normality and homogeneity of variance.\n",
        "Bayesian Approach: The Bayesian approach also relies on assumptions, but these are incorporated in the form of prior distributions and likelihood functions. Bayesian methods can be more flexible in handling violations of classical assumptions. For instance, if the data are not normally distributed, the Bayesian approach can incorporate nonparametric models or use robust priors to better account for the data structure. Furthermore, Bayesian methods can provide a more direct way of incorporating uncertainty due to assumptions by updating the beliefs about the model through the posterior distribution.\n"
      ],
      "metadata": {
        "id": "YYvrvZ1U3_aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8: You have two sets of data representing the incomes of two different professions1 V Profession A: [48, 52, 55, 60, 62' V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test? Task: Use Python to calculate the F-statistic and p-value for the given data. Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.**"
      ],
      "metadata": {
        "id": "fbIH66Z-3_fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "# Income data for Profession A and Profession B\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate the variances\n",
        "variance_a = np.var(profession_a, ddof=1)  # ddof=1 for sample variance\n",
        "variance_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "# Calculate the F-statistic\n",
        "f_statistic = variance_a / variance_b\n",
        "\n",
        "# Degrees of freedom\n",
        "df1 = len(profession_a) - 1\n",
        "df2 = len(profession_b) - 1\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 1 - stats.f.cdf(f_statistic, df1, df2)\n",
        "\n",
        "# Print the results\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: Variances are not equal.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: Variances are equal.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezXsj8LX6aX4",
        "outputId": "993881f3-1b36-4d90-d86e-344a70eaeb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "P-value: 0.24652429950266952\n",
            "Fail to reject the null hypothesis: Variances are equal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data\n",
        " Region A: [160, 162, 165, 158, 1640]\n",
        "Region B: [172, 175, 170, 168, 174]\n",
        "Region C: [180, 182, 179, 185, 183]\n",
        " Task: Write Python code to perform the one-way ANOVA and interpret the results V\n",
        "Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.**\n"
      ],
      "metadata": {
        "id": "aqyEQyER3_iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Height data for three regions\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Print the results\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There are significant differences in average heights between regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There are no significant differences in average heights between regions.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goEbZrHj6xQe",
        "outputId": "ff73b655-59b8-41c1-e17b-52deb653a1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "P-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis: There are significant differences in average heights between regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KpmK0SDH62Dp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}